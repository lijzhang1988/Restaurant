# coding = utf-8

import MeCab
from pandas import read_csv
import pandas as pd 
import csv
import re
import unicodedata


def get_word(words):
    """
    """
    mecab = MeCab.Tagger ('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd') # Japanese dictionary path
    #mecab = MeCab.Tagger ('')
    # text = 'æ—¥æœ¬ã€ã€’171-0022 æ±äº¬éƒ½è±Šå³¶åŒºå—æ± è¢‹ï¼‘ä¸ç›®ï¼‘ï¼–âˆ’ï¼‘ï¼˜ ãƒ•ã‚§ã‚¤ã‚¹æ± è¢‹4F'
    stop_word = read_csv('stop_word.csv',sep=',')
    stop_word = stop_word.values
    # text = "ä½•ã‚ˆã‚Šå€‹å®¤ã§ã‚†ã£ãŸã‚Šã¨ã—ãŸæ™‚é–“ã‚’éã”ã›ãŸã®ã§ã€ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã¨ã¯é•ã£ã¦å±…é…’å±‹ã‚‰ã—ãä¼šè©±ã¨ãŠé…’ã¨é£Ÿäº‹ã‚’æ¥½ã—ã‚€ã“ã¨ãŒã§ãã¾ã—ãŸï¼å‹é”ã¨äºŒäººã§å¤§æº€è¶³ã§ã™â™ª"
    output_words = ''
    keywords = mecab.parse(words)
    for row in keywords.split("\n"):
        word = row.split("\t")[0]
        if word not in stop_word:
            if word == "EOS":
                break
            else:
                #print(word)
                pos = row.split("\t")[1].split(",")[0]
                #pos = row.split("\t")[1].split(",")[1]  
                #print(pos)  
                #if pos in ['è‡ªç«‹', '*', 'å½¢å®¹å‹•è©èªå¹¹', 'éè‡ªç«‹', 'åŠ©è©é¡æ¥ç¶š', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½', 'ä¸€èˆ¬']:
                #if pos in ['åè©','å‹•è©','å‰¯è©','å½¢å®¹è©','åŠ©å‹•è©']:
                if pos in ['åè©','å‹•è©']:
                    output_words += str(word) + ' ' 
    return output_words
    
    """
    output_words = ''
    keywords = mecab.parse(words)
    for row in keywords.split("\n"):
        word = row.split("\t")[0:]
        if word == "EOS":
            break
        else:
            print(word)
            #pos = row.split("\t")[1:].split(",")[0:]    
            #if pos in ['åè©', 'å‰¯è©', 'å‹•è©', 'å½¢å®¹è©', 'åŠ©è©', 'åŠ©å‹•è©']:
            output_words += str(word) + ' ' 
    return output_words
    """

def unicode_normalize(cls, s):
    pt = re.compile('([{}]+)'.format(cls))

    def norm(c):
        return unicodedata.normalize('NFKC', c) if pt.match(c) else c

    s = ''.join(norm(x) for x in re.split(pt, s))
    s = re.sub('ï¼', '-', s)
    return s

def remove_extra_spaces(s):
    s = re.sub('[ ã€€]+', ' ', s)
    blocks = ''.join(('\u4E00-\u9FFF',  # CJK UNIFIED IDEOGRAPHS
                      '\u3040-\u309F',  # HIRAGANA
                      '\u30A0-\u30FF',  # KATAKANA
                      '\u3000-\u303F',  # CJK SYMBOLS AND PUNCTUATION
                      '\uFF00-\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS
                      ))
    basic_latin = '\u0000-\u007F'

    def remove_space_between(cls1, cls2, s):
        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))
        while p.search(s):
            s = p.sub(r'\1\2', s)
        return s

    s = remove_space_between(blocks, blocks, s)
    s = remove_space_between(blocks, basic_latin, s)
    s = remove_space_between(basic_latin, blocks, s)
    return s

def normalize_neologd(s):
    s = s.strip()
    s = unicode_normalize('ï¼-ï¼™ï¼¡-ï¼ºï½-ï½šï½¡-ï¾Ÿ', s)

    def maketrans(f, t):
        return {ord(x): ord(y) for x, y in zip(f, t)}

    s = re.sub('[Ë—ÖŠâ€â€‘â€’â€“âƒâ»â‚‹âˆ’]+', '-', s)  # normalize hyphens
    s = re.sub('[ï¹£ï¼ï½°â€”â€•â”€â”ãƒ¼]+', 'ãƒ¼', s)  # normalize choonpus
    s = re.sub('[~âˆ¼âˆ¾ã€œã€°ï½]', '', s)  # remove tildes
    s = s.translate(
        maketrans('!"#$%&\'()*+,-./:;<=>?@[Â¥]^_`{|}~ï½¡ï½¤ï½¥ï½¢ï½£',
              'ï¼â€ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¿¥ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ã€œã€‚ã€ãƒ»ã€Œã€'))

    s = remove_extra_spaces(s)
    s = unicode_normalize('ï¼â€ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼Ÿï¼ ï¼»ï¿¥ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ã€œ', s)  # keep ï¼,ãƒ»,ã€Œ,ã€
    s = re.sub("[a-zA-Z0-9]","",s)
    s = re.sub("[\s+\.\!\/_,$%^*(+\"\'ï¼›ï¼šâ€œâ€ï¼]+|[+â€”â€”ï¼ï¼Œãƒ»ï¼Ÿ?ã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â—)â€¼â­âœ¨)ã€‚â˜†;)ã€“:â¤ğŸ’¨Ğ´â™ªâ‰âš¡Â´âˆ€`ã€‡ï¼-ğŸ˜„ğŸµâ€»-â– â–¡ã‚œ+.â™ªãƒ«ãƒ³ãƒ¾(â—'âˆ€'â—)ãƒãƒ«ãƒ³â™ª.-â˜…ï¿½]+", "",s) 
    return s


if __name__ == '__main__':
    sourceFile = 'two_other_2.csv'
    targetFile = 'two_other_cut2.csv'
    #names = ['comment']
    data_neg = read_csv(sourceFile,sep=',')
    #print(data_neg[0])
    print(data_neg.shape)
    #print(data_neg.head(5))
    print(data_neg.describe())
    x = data_neg.values
    y = data_neg.iloc[:,5]
    #print(x[29])
    print(x.shape)
    txt = y[0]
    print(str(txt))
    
    with open(targetFile, 'w', newline='',encoding='utf-8') as csvf:
        fieldnames = ['comments']
        writer = csv.DictWriter(csvf, fieldnames=fieldnames)
        #writer.writeheader()
        for i in y:
            i = normalize_neologd(i)
            if pd.isna(i):
                i = 'æœ¬ç¤¾'
            res=get_word(str(i))
            writer.writerow({'comments': res})
          